% !TEX root = ./article.tex

\documentclass{article}

\usepackage{mystyle}
\usepackage{myvars}

%-----------------------------

\begin{document}

	\maketitle
  \thispagestyle{empty}

%-----------------------------
%	TEXT
%-----------------------------

  \section{Demostración de la Insesgadez}
  \label{sec:demostration_1}

    \paragraph{}
    Para una población población finita $U$ formada por los individuos $\{ 1, ..., i, ..., N\}$ cuya variable de interés se denota por $y$, siendo $y_k$ el valor que toma dicha variable en el individuo $k$. Se define el total poblacional como sigue:
    \begin{align}
      t &= \sum_{k \in U}y_k
    \end{align}

    \paragraph{}
    Sea $s$ una muestra extraida de la población $U$ (por tanto $s \subset U$), se define la variable aleatoria $I_k$ como se indica a continuación:

    \begin{equation}
      I_k =
      \begin{cases}
        1 & \text{si } k \in s \\
        0 & \text{si } k \not\in s
      \end{cases}
    \end{equation}

    \paragraph{}
    De esta manera, se puede utilizar el valor $\pi_k$ para modelizar la probabilidad de que $I_k$ tome el valor $1$, es decir, de que el elemento $k$-ésimo esté en la muestra $s$. Por tanto, se puede afirmar que $I_k \sim B(\pi_k)$, una distribución de \emph{Bernoulli} de parámetro $\pi_k$. A partir de dicha distribución se tiene que $E[I_K] = \pi_k$. Además, $pi_k > 0 \forall k \in U$ para que se cumpla la restricción de muestreo probabilístico.


    \paragraph{}
    Se define el $\pi$-estimador como:

    \begin{equation}
      \widehat{t}_\pi = \sum_{k \in s} \frac{y_k}{\pi_k} = \sum_{k \in U} I_k \frac{y_k}{\pi_k}
    \end{equation}

    \paragraph{}
    La demostración acerca de la insesgadez de este estimador deriva de la esperanza de la variable $I_k$, que toma el valor $\pi_k$, tal y como se indicaba anteriormente, lo cual produce que se anule dicho valor $pi_k$ con el $pi_k$  del divisor del estimador. Además, la restricción de muestreo probabilístico $pi_k > 0 \forall k \in U$ hace que el valor $\widehat{t}_\pi$ exista siempre (no hay división entre 0). Por estas razones se puede decir que el estimador es insesgado. De manera matemática esto se describe a continuación:

    \begin{align}
      E[t] &= \sum_{k \in U} y_k \\
      E[\widehat{t}_\pi] &= \sum_{k \in U} E[I_k] \frac{y_k}{\pi_k} = \sum_{k \in U} \pi_k \frac{y_k}{\pi_k} =\sum_{k \in U} y_k \\
      sesgo(t, \widehat{t}_\pi) &= \left| E[t] - E[\widehat{t}_\pi] \right| = \left| \sum_{k \in U} y_k - \sum_{k \in U} y_k \right| = 0
    \end{align}

    \paragraph{}
    Nótese por tanto, que el funcionamiento del $\pi$-estimador se basa en la \say{expansión} del conjunto de individuos seleccionados en la muestra $s$, de tal manera que se siga cumpliendo de la forma más precisa posible la distribución de probabilidades de la población $U$.

  \section{Demostración del Intervalo de Confianza}
  \label{sec:demostration_2}

    \paragraph{}
    En este caso se pretende probar el intervalo de confianza de nivel $1-\alpha$ para la distribución de probabilidades referida a la variable aleatoria $t$, es decir, a la suma del total poblacional para la variable $Y$. Es decir, se pretende probar la igualdad descrita en la ecuación \eqref{eq:confidence_interval}. Dicha demostración se llevará a cabo siguiendo la notación de valores de probabilidad.

    \begin{align}
    \label{eq:confidence_interval}
      \left[\widehat{t}_\pi \pm z_{1-\alpha/2}\sqrt{\widehat{Var}[\widehat{t}_\pi]}\right] &\equiv \Pr\left(t\in \left[\widehat{t}_\pi \pm z_{1-\alpha/2}\sqrt{Var[\widehat{t}_\pi]}\right]\right) = 1-\alpha
    \end{align}

    \paragraph{}
    Un paso previo para la demostración del intervalo de confianza es la demostración de la ecuación \eqref{eq:normal_asumption}. Por el \emph{Teorema Central del Límite} se puede asumir que la distribución $t$ sigue una distribución normal con media $\widehat{t}$ y varianza $Var[\widehat{t}_\pi]$. Por tanto, este puede ser tipificado a una distribución normal estándar. Esta propiedad se indica en la ecuación \eqref{eq:normal_asumption}

    \begin{align}
    \label{eq:normal_asumption}
      \frac{t - \widehat{t}_\pi}{\sqrt{Var[\widehat{t}_\pi]}} \sim N(0,1)
    \end{align}

    \paragraph{}
    Por tanto, para demostrar que el intervalo de confianza sigue un nivel $1-\alpha$ basta con aplicar distintas propiedades algebraicas sobre este, hasta llegar a la distribución normal estándar tal y como se indicó en la ecuación \eqref{eq:normal_asumption}, que debido a su simetría permite sustituir el intervalo multiplicando por $2$ el valor de $\alpha$. Por último, debido a las propiedades de la distribución normal estándar, queda demostrado que en hasta el percentil $z_{1-\alpha}$ se localiza la probabilidad $1-\alpha$.

    \begin{align}
      \Pr\left(t\in \left[\widehat{t}_\pi \pm z_{1-\alpha/2}\sqrt{Var[\widehat{t}_\pi]}\right]\right) &=\\
      &= \Pr\left(t - \widehat{t}_\pi \in \left[\pm z_{1-\alpha/2}\sqrt{Var[\widehat{t}_\pi]}\right]\right) \\
      &= \Pr\left(\frac{t - \widehat{t}_\pi}{\sqrt{Var[\widehat{t}_\pi]}} \in \left[\pm z_{1-\alpha/2}\right]\right)  \\
      &= \Pr\left(N(0,1) \in [\pm z_{1-\alpha/2}]\right) \\
      &= \Pr\left(N(0,1) \leq z_{1-\alpha}]\right) \\
      &=  1- \alpha
    \end{align}
%-----------------------------
%	Bibliographic references
%-----------------------------
	\nocite{muest2017}
  \nocite{sarndal2003model}

  \bibliographystyle{alpha}
  \bibliography{bib}

\end{document}
